{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLfRHQQslu8RKtfV9UoJS6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swethasrihari/NLP/blob/main/NLP_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT"
      ],
      "metadata": {
        "id": "gjG3nj9eLM9z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djqPlFG3n0SJ",
        "outputId": "7fbbd94b-f83c-4afd-82e1-fde999c6479e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmIpAV8Sn5Jg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "training_path ='/content/drive/MyDrive/Colab Notebooks/sentiment_train.json'\n",
        "test_path = '/content/drive/MyDrive/Colab Notebooks/hw2_sentiment_test.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-prCuwippcS"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_json(training_path, lines=True)\n",
        "test_data = pd.read_json(test_path, lines= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3nYDpcGrAah"
      },
      "outputs": [],
      "source": [
        "def extract_required_columns(dataset):\n",
        "  stars = []\n",
        "  review_title = []\n",
        "  for record in dataset.values:\n",
        "    stars.append(record[3])\n",
        "    review_title.append(record[5])\n",
        "  return stars,review_title\n",
        "\n",
        "stars, review_title = extract_required_columns(train_data)\n",
        "test_stars, test_review_title = extract_required_columns(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho104VnBAmvm"
      },
      "source": [
        "Loading the data and labels into data frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rPyhhspya4h"
      },
      "outputs": [],
      "source": [
        "data = {\"review_title\":list(review_title),\n",
        "        \"stars\": stars}\n",
        "df = pd.DataFrame(data)\n",
        "testing_data = {\"review_title\":list(test_review_title),\n",
        "                \"stars\":test_stars}\n",
        "test_df = pd.DataFrame(testing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw1_0_9ptigf",
        "outputId": "8d8cfc65-c841-4de5-f6a0-47db18eb3a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from Transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from Transformers)\n",
            "  Downloading huggingface_hub-0.19.1-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.1/311.1 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from Transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from Transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from Transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from Transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from Transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from Transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from Transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->Transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->Transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from Transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, Transformers\n",
            "Successfully installed Transformers-4.35.0 huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1\n"
          ]
        }
      ],
      "source": [
        "pip install Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D__9utCVAMGW"
      },
      "source": [
        "Using the BERT tokenizer for padding and converting the review titles into tokens that the model can understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268,
          "referenced_widgets": [
            "c0300a532d3a43a395af898de1f79e2e",
            "b9348dcfe967401d88ebb682168b15a0",
            "4f3f519930bf44b5942ec53c3be7a79b",
            "ea49435a46af41d1b80485db642e5db3",
            "bc6051e12a30441dbbd5c88b9c2c3aeb",
            "84c6aa4945d64b12918ab7dc9fd92e72",
            "1d2504b02488439ea3c734c70adbc195",
            "b19bf582a8fb4dbf8d08311be60b19a3",
            "af526427ed9d4cce9dad0de3ae42d451",
            "26ab0c407ad34f4cb809ac9305e0f712",
            "b7850e013fdb478f81b4e470f2f76964",
            "8339397af056483fb3f4653a91736781",
            "93cbbaa9d4954e18915a6acef316bd62",
            "7d8376f44c2146f0af8b3d47f0964917",
            "49336d825ba245f8ad0a631c36ce04a9",
            "4c1b18c959ce49e3ae1889e4e50837e3",
            "e791221627a445288cfb6b6ccef2bf9b",
            "38aff45142f94f93b0f1c0d6e344d3dd",
            "752d68b30447488eaf17472f4c99f6cb",
            "088b76232922471babb20560ed157b7e",
            "f7a3c7a62a59431887607bf0466796a1",
            "814679f085b64643933f68156ebfd191",
            "502d80f7798d44008744c08e55cddeae",
            "82dd9bb4203847ac86147a8647c69984",
            "145f87c75b754a07b11b30457a097aa2",
            "d9f2f558b264455dbacd7cb65323cc5f",
            "da3e5f15e5bc4d6bb0fd46daf3273e91",
            "6ac4793fe89e449382ff84f692a12ce9",
            "bab3bd536e6043a395415667f275c4f1",
            "9efc03a63d5e487abb32d8adb14ad1c4",
            "26aa00a6129e46ca986e3f988fe03ba0",
            "d22e7d6a4310436a801b5dd713601c3b",
            "1a97d559af4e4f1fa648e133f4ca18a3",
            "7f2b835313704726851d4195484cab2b",
            "44cf75342d1248e4babb0939935577a5",
            "ae1a970c9edb44d39f926e6fcbf04f5e",
            "53981f5bf2da49ea8b17b9de8660322a",
            "83de29c6594342f6b8df8c4ce02d1424",
            "345e48a6d69548f3978b46322775ad67",
            "2251f671df014885b6884630307a3d27",
            "c28be374acd04f3a99619e3e7b4c83cb",
            "db3c31c8ae4440ee9eda1548bc11595a",
            "f5b554e3469a4c46af93e9b3fb674464",
            "b9077fc4370c495a9eb49209fc687282",
            "adb21528c8834f5daad9c6055b6c6588",
            "37941dd09dbe433496af231838037bbe",
            "59cf7d7d9a7d4534a841e7b0eab849f1",
            "537dab8f240048239de6e8e71ac1f0d6",
            "7ab97f5c1ec141f6b57953a17f8f86e7",
            "1be2e7d8ee794563a8f9f99aec0cf119",
            "baee3ca02d5b406db25148e677d447d2",
            "20a347ce95d2433e904b158887fbf6b9",
            "2664938465b24bbdb3ec6b99f5e7f662",
            "090963bd6dcb45e6b5c4ff809dbd5a4d",
            "db5a52ab1f614a9487e86512a9515b6d"
          ]
        },
        "id": "AVtvxqCasAG0",
        "outputId": "26c03be4-535c-456a-ccbb-6e8503fb59eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0300a532d3a43a395af898de1f79e2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8339397af056483fb3f4653a91736781"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "502d80f7798d44008744c08e55cddeae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f2b835313704726851d4195484cab2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adb21528c8834f5daad9c6055b6c6588"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers.models.bert.modeling_tf_bert import TFBertForTokenClassification\n",
        "import transformers\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForTokenClassification.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ_COrVH0XZo",
        "outputId": "250d6171-21b2-4745-aae1-2be2af6abaa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80000, 3)\n"
          ]
        }
      ],
      "source": [
        "# tokenizing the review titles\n",
        "import numpy as np\n",
        "df['tokenized_review'] = df['review_title'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=120, padding='max_length', truncation=True))\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['tokenized_review'] = test_df['review_title'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=120, padding='max_length', truncation=True))\n",
        "print(test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLmpGbJh8zAw",
        "outputId": "0082b485-547f-4cef-a655-4623b5c3d2c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset into training and validation datasets"
      ],
      "metadata": {
        "id": "L2Bz2uFhJSEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(df['tokenized_review'], df['stars'], test_size=0.2, random_state=42)\n",
        "\n",
        "print(len(X_train), len(y_train))\n",
        "print(len(X_val), len(y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55rbQtHsg_1V",
        "outputId": "43985c02-2ea3-4dc0-e318-46777a5bc5d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64000 64000\n",
            "16000 16000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([np.array(value) for value in X_train])\n",
        "x_val = np.array([np.array(value) for value in X_val])\n",
        "x_test = np.array([np.array(value) for value in test_df['tokenized_review']])\n"
      ],
      "metadata": {
        "id": "ut8kbVXk-leT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {1:0, 5:1}\n",
        "y_train = y_train.map(labels)\n",
        "y_test = test_df['stars'].map(labels)\n",
        "y_val = y_val.map(labels)"
      ],
      "metadata": {
        "id": "Qq-5RlR4RL4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEEYCumJtSU-",
        "outputId": "4da99c36-37b8-4de7-a95a-8f8cbeb1a418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
        "    loss=tf.keras.losses.binary_crossentropy,\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "metadata": {
        "id": "tbDE1raNtTNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=x_train, y=y_train, epochs=1, batch_size=32, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "5jm8DMyDtYL3",
        "outputId": "663b4157-b7b7-4ec4-8aab-51e79dc59cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 1669s 814ms/step - loss: 3.9762 - accuracy: 0.8263 - val_loss: 4.0134 - val_accuracy: 0.8864\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c7be81c4bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_labels = np.argmax(y_pred.logits, axis=1)"
      ],
      "metadata": {
        "id": "gxd4Ok_ivRij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test, y_pred_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWMTHdJH3Z5t",
        "outputId": "aa74827a-3415-41e7-82eb-24924b7683e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.80      0.89      1000\n",
            "           1       0.83      0.99      0.91      1000\n",
            "\n",
            "    accuracy                           0.90      2000\n",
            "   macro avg       0.91      0.90      0.90      2000\n",
            "weighted avg       0.91      0.90      0.90      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Confusion Matrix : \")\n",
        "print(confusion_matrix(y_test, y_pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGutDehx_H2r",
        "outputId": "fc20489f-8a84-4924-a115-15cb5ba4e9bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix : \n",
            "[[801 199]\n",
            " [  8 992]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open AI's GPT-2"
      ],
      "metadata": {
        "id": "PLs7Y045yg0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing the review titles\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "df['tokenized_review'] = df['review_title'].apply(lambda x: tokenizer.encode(x, max_length=120, truncation=True))\n",
        "test_df['tokenized_review'] = test_df['review_title'].apply(lambda x: tokenizer.encode(x, max_length=120, truncation=True))"
      ],
      "metadata": {
        "id": "Rtm89kSRe8Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(df['tokenized_review'], df['stars'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "xTxnTM4g-1Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting into numpy array and padding to ensure that the sequences have the same length\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=120)\n",
        "x_val = tf.keras.preprocessing.sequence.pad_sequences(X_val, padding='post', maxlen=120)\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(test_df['tokenized_review'], padding='post', maxlen=120)"
      ],
      "metadata": {
        "id": "d1n8IAKC-8Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting labels to 0 or 1\n",
        "labels = {1: 0, 5: 1}\n",
        "y_train = y_train.map(labels)\n",
        "y_test = test_df['stars'].map(labels)\n",
        "y_val = y_val.map(labels)"
      ],
      "metadata": {
        "id": "SouSFobv_nnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-2 model and add a classification head on top of the model\n",
        "model = TFGPT2Model.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
        "inputs = tf.keras.layers.Input(shape=(120,), dtype=tf.int32)\n",
        "outputs = model(inputs)['last_hidden_state']\n",
        "pooled_output = tf.reduce_mean(outputs, axis=1)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(pooled_output)\n",
        "classification_model = tf.keras.models.Model(inputs=inputs, outputs=output)"
      ],
      "metadata": {
        "id": "uyIWZE_j_zzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and train the model\n",
        "classification_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
        "    loss=tf.keras.losses.binary_crossentropy,\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "Zhhi12F4AIun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "ffd3197fe2624aa7a1142bae126a396a",
            "6b86922698094843bd4a6a0623f6a85d",
            "73532e2f8b414bedb6c63b15b794fd75",
            "e5cfa897c1b04cc1936ef1b84fe6f870",
            "a1ad77d434cb41a7b9f95566400139d6",
            "b2f26d01d27d49fb91f0a922e2aaa01d",
            "65d5543e1dd0477691ce4ca278114088",
            "a78f99cf4a1b4f868273cd803ab21ca4",
            "dcbabea1fc6e4974933d5666f14b262b",
            "a31df213ee96441ba72832368f88a40f",
            "44c07aaa1daa4ac886bc39f61ea84fb1",
            "5db5ecc60b0246d8b520155ef609eb24",
            "5188b9eae43a418290b1335ba2c877d9",
            "7b571006c61e4189ae6543c233a9a549",
            "b667dc8845a6460a8cb37d2f175099e3",
            "58526e596fc14157b57e7fc44ca16da6",
            "484c4f4ef3644401ac23e42c2de3da18",
            "1c6382f336304b6c83096a9a1635b269",
            "6d0ec191d3d64d89a6fd32f2996288db",
            "6826778878c1430989a0dff1aa86cfb1",
            "c26f41b2d2fe43f990e7e87707d17f0f",
            "f009348f6d924f859e18a43e22f633d9",
            "92f372560f09458aadf62acc9c9b581b",
            "1ffe91785ed74006a995cbd7e3fd1cc8",
            "9328d180fccc415886aa96b3b0244f1a",
            "a45ec1df7a034e4bacb49d2bc20549cb",
            "d6f8ba9c54b542bfaee9e67d6edfe2df",
            "b4d29ff0c0cc44fb86334c4f936f6ef9",
            "5817a1a061c84bd6a852f6af8ff9ff3c",
            "5c420f3a974d44948280be655129dc74",
            "cd60310b0f754065ae63deb9f6b94d6f",
            "d5e7b0614dab4125903b3799469fda5b",
            "45a732e5705b47c3b15a5d440c7ad2b7",
            "d7dce06f455e4259af08b3f6a2648328",
            "8066b7f9a97a45e4a5a38dfd6dcda729",
            "9d59930041a743bebc8ea8eb632b25a8",
            "9cd28effce7144b29df1d70e07336554",
            "6c3c41d48cb54418b79ed6ca72248a27",
            "7d31b44bbced48f78eb4c44f9762ff19",
            "5eb01f98f5594ba286e33c23d5b890d3",
            "52119ef9590d4085ac566f832e8494ee",
            "a8012041263f438e8fe52012fa9f3248",
            "ff8a0c3ac2d9475ab619cc7138619c9c",
            "96831a9a165f43b6bde169d46425052c",
            "b1f9948947b149f7af14b03a6a55f6a9",
            "07301a1d88464a9081d3597d7ae20734",
            "d2cd12afef574b41af9f7d0438e0642a",
            "9ee39af2348f4acba16d22f35d37c4d2",
            "69fda35ebaea4401b9c4e3c4f323ca39",
            "00ac363522c24f829ac61c8e89bb3e43",
            "b8987df8ca0245479c2a41045b331864",
            "be42b53f8904424898c542418aedfcf9",
            "1ae5ed13f33e49f3bd563f3cf747f729",
            "16ba913f749f430a8079819e5e15b93f",
            "514ec8d8948f422da4c93d30e90cfb77"
          ]
        },
        "id": "qQdSexcVxyGA",
        "outputId": "e9eb480b-4914-4a57-99b8-41ac6dc163cb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffd3197fe2624aa7a1142bae126a396a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5db5ecc60b0246d8b520155ef609eb24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92f372560f09458aadf62acc9c9b581b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7dce06f455e4259af08b3f6a2648328"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1f9948947b149f7af14b03a6a55f6a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2Model.\n",
            "\n",
            "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 1700s 835ms/step - loss: 0.2052 - accuracy: 0.9129 - val_loss: 0.1457 - val_accuracy: 0.9439\n",
            "63/63 [==============================] - 19s 264ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.96      0.94      1000\n",
            "           1       0.96      0.92      0.94      1000\n",
            "\n",
            "    accuracy                           0.94      2000\n",
            "   macro avg       0.94      0.94      0.94      2000\n",
            "weighted avg       0.94      0.94      0.94      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classification_model.fit(x=x_train, y=y_train, epochs=1, batch_size=32, validation_data=(x_val, y_val))\n",
        "# Predict and evaluate\n",
        "y_pred_probs = classification_model.predict(x_test)\n",
        "y_pred_labels = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "report = classification_report(y_test, y_pred_labels)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Confusion Matrix : \")\n",
        "print(confusion_matrix(y_test, y_pred_labels))"
      ],
      "metadata": {
        "id": "Qvw9TKWryttl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6eb1c25-7cbc-4717-ceb0-ae5ab0204ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix : \n",
            "[[959  41]\n",
            " [ 76 924]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing the performance of the BERT and GPT-2 models\n",
        "\n",
        "* When you compare the confusion matrix of the two models, I noticed that BERT predicted very little false negatives for each class when compared with GPT-2. But it gave more false possitives. For true possitives, both the models did well for one of the classes.\n",
        "\n",
        "* I also noticed that BERT produced 99% precesion for 0 and 99% recall for 1 compared to 93% and 92% for the GPT2 model.\n",
        "This indicates that the BERT model has a low rate of false positives for class 0. 99% recall means the model is very good at capturing the positive instances of class1.\n",
        "\n",
        "Overall I think both models performed equally but BERT slightly better at classifying even though the accuracy score if only 90%.\n",
        "\n",
        "\n",
        "## preprocessing, training and challenges\n",
        "\n",
        "* I have not preprocessed the data like the previous assignments.\n",
        "\n",
        "* I have used the model's tokenizer to do the preprocessing, tokenizing, padding and truncating.\n",
        "\n",
        "* I have set the maximum length of the tokens to be 120.\n",
        "\n",
        "* The entire dataset with 80000 rows split into training and validation sets was used for trainig both the models.\n",
        "\n",
        "* Initially while training with 10000 rows for BERT, the accuracy was 100%. Since the model was getting overfit, I was playing around with the amount of data for training. And fortunately I saw that feeding more data made some difference. I could reduced the dataset a bit more and try to optimize.\n",
        "\n",
        "* For the BERT model no classification head was added. But for GPT-2 I had to add a classification head due to mismatch in input shape.\n",
        "\n",
        "* I was stuck with GPT2 for days throughing me all sorts of error but finally I figured that the problem was with the way I was converting the data into numpy arrays. The following suggestion from chatGPT help with all my errors.\n",
        "tf.keras.preprocessing.sequence.pad_sequences\n",
        "\n",
        "* I had also faced \"out of memory\" error a couple of times which was not related to GPU. I didn't understand that error and how it vanished.\n",
        "\n",
        "* I had to switch multiple times between my student account and personal account for training the models since I ran out of GPU very quickly.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xEag_PzMCnlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resources used\n",
        "\n",
        "https://huggingface.co/bert-base-uncased\n",
        "\n",
        "https://stackoverflow.com/questions/74914230/how-to-import-transformers-with-tensorflow\n",
        "\n",
        "https://huggingface.co/bert-base-uncased\n",
        "\n",
        "ChatGPT"
      ],
      "metadata": {
        "id": "BFYK5j6KO_uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to pdf \"/content/drive/MyDrive/Colab Notebooks/NLP_HW3.ipynb\"\n",
        ";"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "3d7zYxg_M1Bt",
        "outputId": "76ef4549-1ec2-4cff-88eb-feb8a2d5f3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/NLP_HW3.ipynb to pdf\n",
            "[NbConvertApp] Writing 59220 bytes to notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 60781 bytes to /content/drive/MyDrive/Colab Notebooks/NLP_HW3.pdf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TV77kzD4NhIX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}